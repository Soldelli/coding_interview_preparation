{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Basic MLP.\n",
        "\n",
        "This is an example code for training a MLP classifier.\n",
        "\n",
        "Questions:\n",
        "1. Implement the fprop and backprop for the LogSoftmax activation function.\n",
        "2. There are a total of 4 bugs in the fprop / bprop of ReLU and Dense Layer. Figure out where those bugs are, and fix them.\n",
        "\n",
        "HINT: There are not any shape errors\n",
        "\n",
        "Below is the equation for Log Softmax:"
      ],
      "metadata": {
        "id": "EWzDgd6enGY3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\large \\textrm{LogSoftMax}(x_{ij}) = \\log\\left(\\frac{\\exp (x_{ij})}{\\sum_{j} \\exp (x_{ij})}\\right)$$"
      ],
      "metadata": {
        "id": "qG7UurA-8r6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3FSOtUY5nQyS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activation Functions\n",
        "\n",
        "class LogSoftmax:\n",
        "\n",
        "  def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "    assert len(x.shape) == 2, \"x is shape (batch_size, in_dim)\"\n",
        "    # raise NotImplementedError(\"Implement Me!\")\n",
        "\n",
        "    max_ = x.max()\n",
        "    term1 = x - max_\n",
        "    term2 = np.log(np.exp(x - max_).sum(axis=1))\n",
        "    logsoftmax = term1 - term2\n",
        "\n",
        "    # num = np.exp(x)\n",
        "    # num /= num.sum(axis=1)\n",
        "    # logsoftmax = np.log(num)\n",
        "    return logsoftmax\n",
        "\n",
        "\n",
        "  def bprop(self, x: np.ndarray, dedy: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    e: error (or loss)\n",
        "    y: fprop output\n",
        "    dedy: de / dy (derivative)\n",
        "    x: input to this layer\n",
        "    \"\"\"\n",
        "    raise NotImplementedError(\"Implement Me!\")\n",
        "\n",
        "\n",
        "class ReLU:\n",
        "\n",
        "  def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "    assert len(x.shape) == 2, \"x is shape (batch_size, in_dim)\"\n",
        "    return np.maximum(x, 0)\n",
        "\n",
        "  def bprop(self, x: np.ndarray, dedy: np.ndarray) -> np.ndarray:\n",
        "    # return x * dedy\n",
        "\n",
        "    dedy[x<0] = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "1iwerjqJnIuZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dense Layer\n",
        "\n",
        "class DenseLayer:\n",
        "  \"\"\"Basic dense layer.\"\"\"\n",
        "\n",
        "  def __init__(\n",
        "    self,\n",
        "    in_dim: int,\n",
        "    out_dim: int,\n",
        "    activation: Optional[Union[LogSoftmax, ReLU]] = None,\n",
        "  ):\n",
        "    \"\"\"Create a dense layer.\n",
        "\n",
        "    Initializes weights with Gaussian and bias zeros.\n",
        "\n",
        "    Args:\n",
        "      in_dim: input dim\n",
        "      out_dim: output dim\n",
        "      activation: if None, no activation (linear projection)\n",
        "    \"\"\"\n",
        "    self.w = np.random.randn(in_dim, out_dim)\n",
        "    self.b = np.zeros(shape=(out_dim,))\n",
        "    self.activation = (lambda z: z) if activation is None else activation\n",
        "\n",
        "  def __call__(self, x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Fprop the layer.\n",
        "\n",
        "    Args:\n",
        "      x: The input of shape (batch_size, in_dim).\n",
        "\n",
        "    Returns:\n",
        "      The output of shape (batch_size, out_dim).\n",
        "    \"\"\"\n",
        "    assert len(x.shape) == 2, \"x is shape (batch_size, in_dim)\"\n",
        "    return self.activation(np.dot(x, self.w)) + self.b\n",
        "\n",
        "  def bprop(\n",
        "    self,\n",
        "    x: np.ndarray,\n",
        "    dedy: np.ndarray\n",
        "  ) -> tuple[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    e: error\n",
        "    y: fprop output\n",
        "    dedy = de/dy (derivative)\n",
        "\n",
        "    da: short for deda, or de / da (derivative)\n",
        "    \"\"\"\n",
        "\n",
        "    x_ = np.dot(x, self.w) + self.b\n",
        "    dedy = self.activation.bprop(x_, dedy)\n",
        "\n",
        "    dx = np.dot(dedy, np.transpose(self.w))\n",
        "    dw = np.dot(np.transpose(dx), dedy)\n",
        "    db = np.sum(dedy, axis=0)\n",
        "\n",
        "\n",
        "    return (dx, (dw, db))\n"
      ],
      "metadata": {
        "id": "aFIpdM8_nPrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kJJrz0HfnDiv"
      }
    }
  ]
}